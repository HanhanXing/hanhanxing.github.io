---
layout: post
title: "从《可解释人工智能导论》开始的领域研习"
date: 2024-08-06
categories: Tech
author:
- NPC毁灭者
- zhenhanxing@outlook.com
meta: 
---

> 一本2022年关于 Explainable AI 的出版物，各种大模型涌现和各种黑盒算法层出不穷的今天有多少模型或者企业在坚持做可解释人工智能？或者这里的可解释人工智能是不是已经因为阻碍了AI的野蛮发展而“过时”了？


# 《可解释人工智能导论》Explainable AI
作者：杨强、等一堆人  
时间：2022年
## 🚧🏗️ 本书大纲
#### 1. 概述——为什么人工智能需要可解释性

#### 2. 挖掘AI的基础理论
- 贝叶斯方法，以及贝叶斯网络学习
- 贝叶斯深度学习
- 贝叶斯网络到可解释的**因果**模型

#### 3. from因果性，to“稳定学习”，和“反事实推理”
- 因果性在机器学习中的定义？
- 稳定学习
- 反事实推理

#### 4. 与或图、和人机协作
- 与或图模型
- 多路径认知
- 人机协作对齐人类认知结构和与或图模型（辅助记忆：“气泡游戏”实验）

#### 5. 深度神经网络的「解释」
> 本章着墨较多，主要由上海交大的人写，夹杂北大、腾讯（？）、港中文、爱丁堡大学的人共同帮忙。干货量尚可。  

- 神经网络可视化
    - 最大激活响应的可视化
    - 网络解剖、特征语义分析
    - 基于反向传播的输入重建可视化
    - CAM / Gard-CAM
- 输入单元重要性归因
    - shap算法
    - 导向反向传播算法
    - 逐层相关性传播算法
    - 积分梯度算法
    - LIME
- 博弈交互解释性理论
    - 沙普利值
    - 博弈交互
    - 随机失活操作、批规范化操作、对抗迁移性、和对抗鲁棒性
- 对表达结构的解释
    - 代理模型解释
    - NLM中语言结构的提取和解释
- 可解释的神经网络
    - 胶囊网络
    - β-变分自编码器
    - 可解释的卷积神经网络
    - 可解释的组成卷积神经网络

#### 6. 【🧑‍⚕️📈💻💬📱】行业应用
> 各章节不同人/机构写就，水平层差不齐哦。  

- 生物医疗（基因编辑、医学影像）   
⭐️⭐️⭐️⭐️ 同济一名年轻博士后兼助理教授写的，还不错
- 金融应用（金融监管、信用评级和预测、等）   
⭐️⭐️⭐️ 赞助机构（华夏基金）员工写的，挺一般
- 计算机视觉应用（很多）   
⭐️ 北航的人写的，很烂很水
- 自然语言处理（很多）  
⭐️ 新加坡国立大学某女子写就，一般，因为有点跑题（她以工具提供为主，而不是方法论介绍），且科普性较低，但提供了不少2年前“很新”的模型分析和模型解释工具。
- 推荐系统的应用（电商、社媒、基于位置的服务、影音平台等）  
无署名作者，奇怪……

#### 7. 结论章节   

## 🗺️🚥 思维路径
### 1. 为什么人工智能需要可解释性
讨论可解释性的必要性，特别是在高风险或关键决策领域（如医疗、金融等）。  
强调透明度和可信度如何帮助增强用户对AI系统的信任。  
### 2. 挖掘AI的基础理论
**贝叶斯方法及网络：** 探索贝叶斯方法在学习和推理中的应用，如何通过贝叶斯网络进行复杂决策过程的建模。  
**贝叶斯深度学习：** 结合深度学习和贝叶斯推理，提高模型的不确定性评估。  
**从贝叶斯网络到因果模型：** 介绍如何从数据中学习因果关系，进而推导出可解释的模型。  
### 3. 从因果性到稳定学习和反事实推理
**因果性定义与应用：** 定义因果性在机器学习中的角色，解释其如何帮助理解模型的决策过程。  
**稳定学习：** 介绍稳定学习的概念，强调在变化环境下保持模型性能的重要性。  
**反事实推理：** 通过构造“如果...会怎样”的场景来分析模型的决策路径和潜在偏差。  
### 4. 与或图与人机协作  
**与或图模型：** 通过与或图展示复杂决策树的结构，使模型决策更加透明（术语：「解释性建模」，「XAI」）。  
**多路径认知：** 探讨如何通过多条路径进行决策，提高模型的鲁棒性。  
**人机协作与认知结构：** 讨论如何将AI模型与人类的认知结构对齐，例如通过“气泡游戏”实验来模拟这一过程。  
### 5. 深度神经网络的解释
**神经网络可视化：** 介绍不同的可视化技术，如激活最大化、网络解剖和CAM/Gard-CAM，以理解神经网络是如何处理信息的。  
**输入单元重要性归因：** 运用shap算法、导向反向传播等方法，分析输入特征对模型预测的影响。  
**博弈论在解释性中的应用：** 通过沙普利值等工具，解释模型决策中各因素的贡献度。  
**对表达结构的解释：** 探索如何使用代理模型和语言模型中的结构提取来解释模型行为。  
**可解释的神经网络架构：** 介绍如何设计可解释的网络结构，如胶囊网络和β-变分自编码器。  
### 6. 行业应用
**生物医疗：** 讨论可解释AI在基因编辑和医学影像中的应用。  
**金融：** 探讨如何利用可解释AI进行风险管理和信用评估。  
**计算机视觉和自然语言处理：** 介绍这些技术在现实世界中的应用，如何通过可解释性提高它们的可用性和安全性。  
**推荐系统：** 分析推荐系统如何利用用户数据生成个性化内容，并确保过程的透明度和公平性。  


## 👉🧠 知识碎片
### I. 概论部份
#### 1. 因果推理的3个层次（by Judea Pearl）：
Association → Intervention → Counterfactuals（**反事实推理**）；  
关联=what i see → 干预=what if i do → 反事实推理=what if i had done。
#### 2. 黑盒模型（对应“事后解释”）和白箱模型（对应“事前解释”）
#### 3. 信任性的「模型」

| T(trust)           | Justified T  | Unjustified T  |
|--------------------|--------------|----------------|
| PT(positive trust) | Justified PT | Unjustified PT |
| NT(negative trust) | Justified NT | Unjustified NT |
|                    | **Reliance, RC** |            |

**总结**一句话，人类对一个系统的「合理正信任」和「合理负信任」越多，这个系统的「可靠性（reliance）」就越高。
### II. 方法部份

#### 1. 贝叶斯
- **贝叶斯网络**是**概率图模型**之一  
- 记忆：贝叶斯网络 → 概率建模 → 求取变量间「相关关系」 → 无法回答“是否”只能回答“大概率会否”  
- 局限：贝叶斯方法（或者更广义的概率方法）只能表示和学习「**变量之间的*相关关系***」，而不是「***因果关系***」（参考**因果推理3层级**理论 by Judea Pearl）
- 所以基于Judea Pearl更推崇“因果推断（Causal Inference）”的问题处理方法，此人推出了一个「因果模型」，借贝叶斯网络的结构，把变量连接关系刻画成“因果”而不是“相关性”……

#### 2. 贝叶斯规划学习
- BPL = Bayesian program learning 贝叶斯规划学习，「学习如何学习」，有很强可解释性（相比于需要大量训练数据的“深度学习”）。

#### 3. 贝叶斯深度学习
这里结合了深度学习和贝叶斯学习的两家之长：
- 第一，**深度学习**对复杂数据的高度**拟合**能力；
- 第二，**贝叶斯方法**对数据**不确定性**的刻画能力。

#### 4. 贝叶斯神经网络 Bayesian Neural Network

#### 5. 稳定学习
- 稳定学习和迁移学习（Transfer Learning）的不同是？
- 稳定学习和深度学习的不同是？
- 稳定学习方法通过「**样本重加权**」的方式，消除了输入变量各维度之间的**相关性**

#### 6. 稳定学习x深度神经网络（结合）：
- 介绍：「***StableNet***」在**图像分类**上的深度稳定学习方法（核心是：通过**RFF 随机傅立叶特征**将原特征投影到高维度空间去，从而消除投影后的特征之间线性相关关系，实现特征之间的独立）  

#### 7. 反事实推理
- 好难懂，但应该是正在兴起的「前沿」  
- 一句话：对预测的预测？即 —— 在潜在结果框架下，预测不同干预对新结果带来的影响  
- 通过构造“如果...会怎样”的场景来分析模型的决策路径和潜在偏差。
- 二值类型干预
- 多位类型干预

#### 8. 与或图
- 与或图长什么样？  
可能看起来像一棵树, 顶部是根节点（代表原始问题）, 下面分支出若干子节点 [其中一些节点会用圆形表示(与节点),另一些用方形表示(或节点)]，从上到下逐层分解, 直到达到最底部叶节点（用特殊符号或形状表示）。
- 与节点（AND节点）表示在该节点下的所有子节点必须同时满足条件（眼+鼻子+嘴巴 = 才是头）
- 或节点（OR节点）表示满足任一子节点的条件即可（鸡冠子 = 多种形状）
- **解释性建模：** 在可解释的AI（XAI）中，与或图模型可以帮助研究者和开发者清晰地理解和展示模型的决策逻辑，尤其是在决策过程复杂或涉及多个步骤和变量的情况下。
- **视觉识别：** 在计算机视觉中，与或图被用来构建对象的层次模型，例如，识别一个场景中的对象时，可以从识别整体到部分，再到具体的特征，每一层处理不同的识别任务。
- **自然语言处理：** 与或图也可以用于解析句子结构，理解句子中的逻辑和语义关系。

#### 9. 与或图模型
- 举例，用于识别图像内容，如人脸、动物、动作。
- 进一步还有「基于与或图的多路径认知」过程，bottom-up和top-down
- 基于与或图结构构建的**神经网络模型**

#### 10. 人类认知结构跟与或图模型的对齐
- why：举例，神经网络判断是否涂口红「可能会」利用弱相关性的「特征」（如，眉型、腮红、长发等），虽然利用这些特征并无不可，但是有「偏见」，**此等「偏见」直接导致了预测结果「不可靠」**

#### 11. 气泡游戏 Bubble Game
- 定义：就是机器对输入图像做基于与或图结构的解析，然后在人类提问后，以“气泡”形式展出原图信息。
- 举例：  
输入：人跑步  
任务：判断图像里人是否在跑步  
USER：“该人是否穿夹克？”  
SYSTEM：“上半身的气泡”  
USER：“说明更多”  
SYSTEM：“提供左臂气泡”  
USER：“右手在干嘛”  
SYSTEM：“提供右臂气泡”  
USER：“该人在跑步”  
结束  
- 规则：机器会根据自己对图像的解析+人类的问题，按照它认为的「特征重要性」优先把重要气泡提供给用户，如果第一次提供气泡不足以回答任务问题，会以此逻辑循环。
- 结论：这个游戏可用于认知对齐程度测试（Misalignment越小=模型与用户认知越对齐）

#### 12. “五心”模型（人工智能x用户之间的认知模型）
- 简介：理解成是气泡游戏的进化版
- 结构：人工智能模型、用户、解释对象（三元）
- 五心，5 minds

    |||
    |---|---|
    |m1|机器自己的知识|
    |m2|用户自己的知识|
    |m12|机器基于对用户的分析理解产生的估计和推理|
    |m21|用户从机器的解释中得到的信息|
    |mc|commend knowledge，机器与用户的共同认知|

- 总的来说，就是misalignment越少，机器的可信度、可靠度评级越高

#### 13. 深度神经网络
- **挥之不去的阴影**：“随着模型复杂性的增加，准确性突飞猛进提升的‘繁荣现象’背后，却有着挥之不去的阴影————神经网络的本质仍是个「**黑盒**」”
- 决策依据「不透明」 == 机器只能回答”大概率会“而不是”是否会“ == 无可信度 == 金融、医疗、公共安全、国安等处于人类理智高地通知的领域

#### 14. 神经网络可视化是什么？目的是？
- 通过可视化来理解和解释神经网络，就这么简单。
- 「**最大激活响应可视化**」  
这是一种用来理解和解释卷积神经网络（CNNs）工作原理的技术。（*CS50 AI里学过自然语言模型的**attention**分布可视化*，本书举例的是图像识别模型的**Activation Response**可视化，不过这俩不一样概念！）
- 目的：**理解内部机制**、**改进模型设计**、**提高透明度**

#### 14.1 关于「注意力机制」Attention和「激活响应」Activation Response的区别x联系
关系：两者都与网络模型如何处理和反应输入数据有关。  
区别：激活响应强调的是神经元对输入的直接反应，而注意力机制则是一种高级的数据加权和选择过程，目的是提取对当前任务最有信息价值的部分。

**激活响应（Activation Response）**   
概念：在神经网络中，尤其是卷积神经网络（CNNs），激活响应指的是网络中特定神经元或神经元层对输入数据的反应强度。激活响应可以通过激活函数（如ReLU、Sigmoid等）的输出值来衡量，这些输出值表示了神经元对输入信号的响应程度。  
目的：激活响应的可视化用于理解和解释模型是如何从输入数据中提取和处理特征的。例如，在视觉任务中，某个卷积层的神经元可能对图片中的边缘或纹理特别敏感。  

**注意力机制（Attention）**   
概念：注意力机制是一种模型设计技术，常用于序列模型如循环神经网络（RNNs）和Transformer模型中。注意力机制使模型能够在处理数据时，根据当前的任务动态地聚焦于输入序列中的重要部分。  
目的：注意力模型通过分配不同的权重（注意力权重）给输入数据的不同部分，从而突出最关键的信息。这有助于提升模型在处理如自然语言处理或复杂场景理解等任务中的表现和效率。  

#### 14.2 网络解剖（Network Dissection）
- 首先，「网络解剖」也是理解和解释深度学习模型的决策过程的方法之一
- 解刨对象，通常是深度神经网络中单个单元（并且主要指**卷积层中的特征检测器**）
- 优点：提供具体的、可解释的**视觉概念（特征语义）**关联，帮助理解单个单元的功能
- 缺点：需要大量预先标注的图像数据（例如，自行车图语义和框选的重复性工作）
- 相比「最大激活响应可视化」方法，可以更精确解刨某个层/某个单元。但是不能像激活响应方法那样，找出“对特定输入数据的敏感性”。  

> *总结：
网络解剖提供了一种系统的方法来解释和量化网络单元与视觉概念的关系，适合深入分析网络的语义理解能力。而最大激活响应可视化则更侧重于提供直观的洞察，帮助开发者快速识别和理解网络对特定输入的敏感性。根据应用的具体需求，可以选择适当的方法或者结合使用这两种方法，以获得更全面的网络理解。*  

#### 14.3 输入重建可视化（基于反向传播）  
> 「基于反向传播的输入重建可视化」与「通过排序具体的输入图片样本来可视化网络特征」的方法不同，主要在于其使用了优化技术来直接构造或修改图像，以达到特定的可视化目标，而非从已有的图片库中选择或排序图片。  

- 关键词：输入重建 x 反向传播（back-propagation）
- 核心步骤：  
    1. 初始化：从一个随机噪声图像或某一基准图像开始。
    1. 定义目标：设定优化目标，如最大化特定层的某个神经元的激活。
    1. 反向传播：通过网络前向传播图像，计算目标激活与实际激活之间的差异，并通过反向传播更新图像的像素值，以减少这一差异。
    1. 迭代优化：重复这一过程，直到达到某个停止条件（如迭代次数、激活强度阈值等）。
- 特点/优缺点：  
    1. 优点1：定制化且灵活（想针对哪个神经元/层就选哪个）；
    1. 优点2：解释更全面，它直接产出的图就是能让该网络「激活响应」度“最”敏感的图，甚至发现训练数据中未曾出现过的模式或特征。
    1. 优点3: 解释更“深入”、“极端”：能够揭示网络对于极端情况的反应，即**理论上最能激活某个神经元的图像是什么样子**
    1. 缺点1: **过拟合**与**抽象性**（产出的图像对人类来说过于抽象或难以理解，因为它们是针对最大化网络响应而优化的，而非自然图像）
    1. 缺点2: **计算成本**高：「反向传播」和「迭代优化」通常需要较高的计算资源（烧钱烧显卡），相比之下，请人一个个框图片素材和打标签可能还便宜点。

#### 14.4 类别激活映射 CAM / Grad-CAM
> CAM（Class Activation Mapping）和 Grad-CAM（Gradient-weighted Class Activation Mapping）是很常见的“图片分类归因法”。

- 场景：图像**分类**、分类工作中的决策依据
- 形式：**热力图**该热力图显示了每个位置对于最终分类决策的重要性。
- 实操：它通过修改网络结构，通常在卷积层之后添加全局平均池化层（GAP），然后连接到一个输出层。最终图片与预测结果最相关的区域会被高亮。
- Grad-CAM：粗暴理解为CAM的升级版
> **联想: CAM方法和输入重建方法的区别？**  
技术实现不同：  
输入重建侧重于通过修改输入图像本身来观察激活变化，侧重于“创造”图像；而 CAM/Grad-CAM 则是通过分析已有的输入和模型的反应来生成热力图，侧重于“解析”现有图像。  
输出类型不同：  
输入重建得到的是修改后的图像本身，可以直观地看到哪些像素被修改以增强特定的响应；CAM/Grad-CAM 输出的是热力图，显示原始输入图像中对模型预测最重要的部分。  

#### 15. 输入单元的重要性归因
- **语义解释**，以下3个在本书被视为「同义词」：
    - 重要性（importance）
    - 归因值（attribution）
    - 显著性（saliency）
- 名词：「**可解释性方法**」，即各种「**算法**」，e.g. 用某某可解释性方法来*计算*输入单元的*重要性*
- 解释算法
    - Shapley Value / SHAP算法
    - GBP（Guided-Back-propagation）导向反向传播算法
    - LRP（Layer-wise Relevance Propagation））逐层相关性传播算法
    - IG（Integral Gradient）积分梯度算法

#### 16. SHAP算法(SHapley Additive exPlanations)是什么？   
> SHAP是一种先进的解释**模型预测**的方法，将博弈论里***沙普利值***（Shapley values）的概念应用于机器学习模型。它提供了一种权衡每个特征对模型预测输出的贡献的系统方式，使得模型的决策过程对人类用户更加透明和可理解。  

- 场景：解释那些复杂的非线性模型，如深度学习网络和随机森林，e.g. 某个体的金融风险评估、医疗诊断依据解释、基于客户行为的用户细分 etc.  
- 核心逻辑：
    - 一句话：利用沙普利值概念解释**每个「输入特征」**对「输出预测结果」的「重要性」  
    - 基于博弈论中的沙普利值：一种公平计算贡献并公平分配游戏中赢利的方法。
    - 深层原理：SHAP 计算**每个特征对模型预测的贡献度**（即，***[`沙普利值`](#17-沙普利值shapley-values)***）。然后它再考虑**所有可能的特征组合**（参考：***[`交互博弈理论`](#19-博弈交互理论game-theoretic-interaction)***），以确保每个特征的贡献被公平地评估。  
    SHAP不仅要解释哪些特征是重要的，还要显示特征是如何影响模型预测的。
- 优点：公平性、局部解释性、泛用性。

#### 17. 沙普利值（Shapley Values）
> 沙普利值是博弈论中的一个重要概念，由劳埃德·沙普利（Lloyd Shapley）于1953年提出。  

- 它是一种「**分配方案**」
- 用于公平地分配*合作游戏/合作博弈*（comparative game）中产生的总收益给各参与者。在这个上下文中，每个玩家的贡献是通过考虑他们在没有其他玩家的情况下能带来多少收益来评估的  
- 计算方式：  
    - 对于每个玩家，考虑其在所有可能的*玩家组合*中添加的价值（*边际贡献*），然后取这些价值的平均。
    - 举例（一个自然语言句子中某个词的Shapley value）

    |input text | different context combos | marginal contribution| 
    |---|---|---|
    |It's a ***charming*** and often affecting journey| XXX a ***charming*** and often XXX journey| 0.1|
    | |It's a ***charming*** and XXX affecting XXX|0.03|
    | |XXX a ***charming*** and XXX journey|0.4|
    | | ... | ... |

#### 18. 其他的「解释算法」
1. **GBP（Guided-Back-propagation）导向反向传播算法**  
它通过修改了传统反向传播的过程，通过只允许正梯度通过ReLU激活函数，忽略负梯度，从而突出显示对模型预测贡献最大的特征。
1. **LRP（Layer-wise Relevance Propagation））逐层相关性传播算法**  
它通过反向传递输出到输入的路径，保持相关性分数的守恒，从而为每个输入特征分配一个“相关性分数”。目的是确定每个特征对于最终决策的重要性。
1. **IG（Integral Gradient）积分梯度算法**  
方法是在基准输入（通常是一个无信息的输入）和实际输入之间进行积分，计算沿这条路径上模型输出相对于每个输入特征的梯度的累积。目的是***量化***输入特征对模型预测的影响。  

##### 18.1 算法对比与分类
1. 对比维度：  
- 模型依赖性：GBP 和 LRP 侧重于特定类型的神经网络，尤其是视觉模型；而 SHAP 和 IG 模型无关，可以应用于任何机器学习模型。  
- 输出解释：GBP 和 LRP 通常用于生成**可视化**的解释，突出显示图像中哪些部分最重要；IG 和 SHAP 提供**数值分析**，**定量**地描述特征的贡献（当然，依旧是可以把定量的数值进行**可视化**）。  
- 计算复杂度：GBP 相对简单，但可能不提供全面的解释；LRP 和 IG 要求较高的计算资源，尤其是 IG 需要在输入空间进行积分；SHAP 虽然理论上计算开销大，但有有效的近似方法，如 TreeSHAP。  

2. 分类：  
- 定性解释：GBP 和 LRP。
- 定量特征重要性：IG 和 SHAP。
- 模型特异性：GBP 和 LRP 针对特定类型网络；SHAP 和 IG 更通用。

#### 19. 博弈交互理论（Game-theoretic Interaction）  
> 本书提及的「***博弈交互解释理论***」在人工智能和机器学习领域，尤其是在模型解释性和决策理论中，指的是如何通过博弈论的原则和方法来分析和理解不同决策实体（如模型中的特征、算法组件或不同模型）之间的相互作用和影响。1999年科学家们在「沙普利值」的基础上提出了这款「**博弈交互指标**」：即，输入单元之间交互作用的量化指标。  

- 双变元博弈交互  
思考两个玩家之间合作与其各自单独工作之间的差值：1+1>2 or 1+1<2 ?   
`I(i,j) = i+j合作贡献值 - i单干贡献值 - j单干贡献值`
- 沙普利相互作用指标  
关注多个输入单元同时存在时，对网络输出的贡献  
`I(脸) = 整张脸的贡献 - 眼睛的贡献 - 鼻子的贡献 - 嘴巴的贡献 - 耳朵的贡献 `
- 多变元博弈交互  
思考4名玩家合作时的总贡献减去4人单独工作时各自的贡献之和  
`B([A]) = 四人合作总贡献 - (1号单干贡献 + ... + 4号单干贡献)`  
由于人与人之间合作，存在「积极交互（合作）」和「消极交互（对抗）」，此法有改良版，此处略。
- 多阶博弈交互  
举例：两个输入单元i,j的m阶博弈交互 = 在m个上下文中，输入单元j的参与对i的贡献值所带来的变化。  
`I(m)(i,j) > 0`，j的参与会增加i的贡献，即i，j是积极交互；
`I(m)(i,j) < 0`，j的参与会减少i的贡献，即i，j是消极交互；
`m阶`，刻画的是交互的上下文复杂度。

##### 19.1 博弈交互的性质/属性
> 目的：博弈论的核心属性有助于分析和设计在人工智能系统中的决策策略。  

1. 可加性（Linearity Property）  
示例：投资组合管理：在金融领域，投资者可能想要知道将两种不同资产组合起来时的预期收益（分别计算并相加2场博弈中的2组沙普利值）。通过使用可加性属性，可以简单地将各自资产的预期收益相加，以估计组合资产的总体表现。
2. 冗余性（Nullity Property）  
示例：团队项目评估：评估每位团队成员的贡献时，如果某个成员的工作与项目成功无关（即他们的工作可被其他成员完全替代，aka，**沙普利值为零**），则在分配奖金或认可时，这个成员的贡献评价应为零。
3. 对称性（Symmetry Property）  
示例：销售奖励：如果两位销售员销售了相同数量的产品，并且他们的销售地区和条件相同，则（**沙普利值相等**的情况下）他们应该获得相等的奖金。
4. 有效性（Efficiency Property）  
示例：收益分配：在一个合作项目中，项目的总收益需要被**完全分配**给所有贡献的合作伙伴。例如，一个跨国合作开发软件的项目，最终的利润需要根据各方的贡献公平地分配给所有参与者。

##### 19.2 博弈交互理论对机器学习的解释
> 本书的特色理论：作者提议使用博弈交互理论来桥接这两个方面，提供一个统一的框架来量化和解释神经网络是如何处理图像中的不同视觉概念的。  

1. 背景： 基于**神经网络的可解释性研究的两大方向**
- 认知层面：研究关注于理解神经网络是如何识别和处理图像中的特征的。这通常包括可视化网络中间层的特征，或者识别出对最终预测结果影响最大的像素点。  
- 数学层面：研究着眼于分析网络的数学性质，比如用信息论来评价网络的泛化能力或鲁棒性（即网络对输入变化的敏感度和在面对未见过的数据时的表现能力）。
1. 结论：博弈交互理论基础能够综合以上两种方向。
- 将复杂的神经网络行为转化为通过博弈交互模型解释的可量化结果（将复杂的语义概念（如形状、纹理）与神经网络的响应相关联，然后用博弈交互理论量化和解释这些概念在网络决策中所扮演角色。）。
- 将视觉概念分为低阶、中阶和高阶，反映了人类对复杂场景理解的层次性。
1. 举例：**用博弈交互理论解释视觉概念的三个层次**
- 低阶博弈交互：涉及简单的局部视觉概念，如边缘或特定的纹理。
- 中阶博弈交互：比如某种特定的模式或对象的部分特征。
- 高阶博弈交互：关注全局的视觉概念，如整个场景的构成或多对象之间的关系。

#### 20.1 随机失活（Dropout）
1. Dropout的本质目的就是防止**过度拟合**。
- 神经网络被迫学习更加鲁棒的特征表示，它不能依赖于任何单一神经元或特定的神经元组合。（其中也有类似于投资组合中风险分散的相似意义在）
1. Dropout为什么能和**博弈交互理论**联系起来？
- 在神经网络中，各个神经元可以视为在进行一个复杂的博弈，每个神经元对最终输出的贡献受其它神经元的状态影响。
- 随机失活使网络不依赖于特定的神经元组合，从而降低了模型的复杂度和过拟合的风险。
- 过拟合现象在某种程度上可以被视为博弈交互的**过度强化**，即模型学习到了训练数据中的特定噪声或细节，而这些特征可能只是在特定样本中偶然出现，不具有普遍性。
- 过拟合 = 过于“完美”，反映出强烈的博弈交互，即**神经元之间过度专门化的协作**，这在面对未见过的数据时往往表现不佳。

#### 20.2 批规范化（Batch Normalization）
1. 规范化 Normalization
- 数据预处理技术，改变输入数据的范围，使其具有**零均值**和**单位方差**，或者将数据**缩放**到特定的最小和最大值之间（如0到1）。
1. 批量规范化 Batch Normalization
- 训练神经网络时使用的特定类型的规范化技术（由Sergey Ioffe和Christian Szegedy在2015年提出）。
- 原理是：先，规范化（同上），再，引入两个可训练参数，缩放因子（γ）和偏移项（β），用于恢复在必要的情况下表示能力。
- 优点是：加速收敛、防止梯度问题、允许更高的学习率、有正则化效果（防止过拟合）
1. 正则化 Regularization
- 防止神经网络模型在训练数据上过度拟合的一些个技术
- 包含：L1正则化（损失函数-权重**绝对值**）、L2正则化（损失函数-权重**平方**和的**惩罚项**）、Dropout（随机失活神经元）、数据增强（如旋转、平移、缩放等）。

#### 21. 对抗迁移性（Adversarial Transferability）、对抗鲁棒性（Adversarial Robustness）
1. 作者意图1：  
安排在这个小节介绍对抗攻击，是想引出“降低‘博弈交互强度’是众多‘经验主义’方法中共有的结果，即 ***降低博弈交互==增强对抗迁移性***”。  
    - 对抗迁移性，指的是对抗样本（即故意设计来欺骗神经网络的输入）在不同模型之间的有效性。
    - “神农尝百草式”的尝试 == 经验主义，非理性主义。

1. 作者意图2：要得出“对抗训练的网络建模了分类性能更强的**低阶博弈交互**，使对抗攻击难以基于这些低阶博弈交互（局部细节）去构建出其他类别的**高阶博弈交互**，因此提高了高阶博弈交互（全局的复杂语义理解）的鲁棒性，从而提升了整个网络的鲁棒性”。  
    - 博弈交互的鲁棒性是如何通过对抗训练提升的？（👆）
    - 对抗鲁棒性，指的是一个神经网络模型能够抵抗对抗样本（恶意输入）攻击的能力。

#### 22. 特征复杂度、模型复杂度、变换复杂度
1. **表征**一致性 Knowledge Consistency  
- 通过英文翻译，得知这是度量两个神经网络是否对相同表征都进行建模的量化值
- 重大意义1: 作为数学工具对explainable AI有重大意义
- 重大意义2: 研究Knowledge Consistency是为了带动Unreliable Feature（不可靠特征）和Blind Spots（知识盲点）的研究。
- 表征=knowledge（用来训练），特征=feature（从输入内容中被提取）
2. 复杂度 Complexity
- **特征复杂度**：  
输入样本的复杂度度量  
e.g. 什么复杂度的特征提取，对图片分类工作的帮助更大？
- **模型复杂度**：  
神经网络本身的复杂度度量（特指是否为不同样本建立了不同的模型）  
- **变换复杂度**（在本书语境中，约等于模型复杂度）：  
神经网络本身，针对不同输入特征的变换种类数量多少的度量  
e.g. 变换复杂度多高的模型，是最适合完成某任务的？

#### 23. 神经网络的「表达结构」
1. 代理模型
    - 搞一个可解释的“代理模型”来模拟神经网络的输出
    - 听起来是多此一举，照葫芦画瓢的玩法？
1. NLM中的自然语言结构 **哈佛CS50AI 里面有直接教学**
    - 传统老办法：Syntactic Parser语法解析器（需要监督）
    - 语法解释：一般是二叉树式的解释和归纳方法
    - BERT、GPT这些大语言模型做语法解释的方法可以理解成“无监督语法解析器”
    - 👆原因是：用卷积神经网络训练，同时把词分割为token，通过学习和预测词之间的“句法距离”，预测和理解词、词组合或句子的意思。
    - PRPM、LSTM、Transformer模型（熟悉的关键词，不赘述了…）

#### 24. 可解释神经网络

##### 24.1 胶囊网络 Capsule Network
Geoffrey E. Hinton等人设计的一个替代「CNN卷积神经网络」的东西，总之可解释性更强。

##### 24.2 β-变分自编码器 β-VAE
- β-Variational Autoencoder
- 首先，Autoencoder、Variational Autoencoder是什么？
- 其次，为什么β-VAE通过“把隐藏层特征映射到可分解的分布形式后对特征之间的关联性进行‘**解耦**’”就能提高对模型的控制度？
- 👆e.g. 生成人像的时候输入“化妆”，由于神经网络隐藏层中「化妆」特征会和「女性」特征在分布上存在耦合，因此输出的人像会倾向于“女性化”，即使我们知道男性也能化妆。

##### 24.3 可解释卷积神经网络（Interpretable CNN/ ICNN）
- 首先，CNN主要用于视觉信息or图片识别/分类
- ICNN本质上是一种特别设计的CNN，核心卖点就是「专门的层，会专门捕捉输入数据的特定特征」  
例如，一些ICNN可能包括专门设计的层，每层只专注于识别图像中的一种类型的视觉特征，如边缘、纹理或形状。尤其在**医疗影像分析**这类需要高度透明决策过程的领域。
- 原理：**滤波器损失函数**  

##### 24.4 可解释组成卷积神经网络 Interpretable Compositional CNN
- 是一种更高级的CNN，它结构上设计成可以识别和组合低级特征以形成更高级的语义概念。
- 更加遵循「博弈交互理论」中「从低阶、中阶到高阶」的模式  
例如，在自动驾驶系统中，一个Interpretable Compositional CNN可以首先识别道路上的基本元素（如车道标记），然后组合这些信息来识别更复杂的场景组成（如道路的布局），最后形成对整个驾驶环境的理解。

#### 25. 滤波器损失函数
*设计一种特殊的滤波器损失函数：每个滤波器只对应于特定类别的特定视觉语义，从而保证每个滤波器学习到的是与特定类别密切相关的、区分性强的特征。*

#### 26. 互信息（Mutual Information）
- 概念：一个来自**信息论**的重要概念，用于衡量两个随机变量之间的相互依赖程度，即一个变量的信息如何减少对另一个变量的不确定性。**克劳德·香农**在1948年提出的概念的一部分。
- 数学定义：两个变量的联合分布与各自独立分布乘积的相对熵。
- 意义：**量化两个变量之间信息共享程度的值**
- 应用：  
    - **特征选择**：上述“滤波器”设计就是借mutual information值的高低，选取预测模型的目标变量和备选特征之间“依赖程度”最高的特征。
    - **图像处理**：互信息常用于图像配准、对齐
    - **生物科学**：例如基因组合和形状表达之间的“依赖程度”、大脑各区域活动和神经模式之间的“关联性”。
- 不同于「热力图」、「散点图」，「互信息-值」可以更加量化数据中的依赖关系。这也是为何在「可解释AI」领域它依旧重要的原因。  
> 原文：“让 输入图像的视觉语义 和 中层特征的视觉语义 的 互信息 增大的过程，就是 神经网络知识表达自省 的过程” （TODO：理解这段话）  

#### 27. 机器视觉
- 视觉关系抽取 visual relationship detection, VRD
- 场景图 Scene Graph
- 视觉推理 Visual Reasoning
- 视觉问答 Visual Question Answering VQA：模型智能水平测试的一个部份
- 基于用户反馈查询修正的交互式框架 IMPR-OVE-QA：局部答案修改+基于反馈自动连续学习能力
- 卷积神经网络CNN中，一般高层网络有大量语义信息但像素低，低层网络有高清像素但语义信息少且细节（fyi
- 生成式对抗网络 GAN （Generative Adversarial Network）：用于P图和扣图

## 🤔❓思考

### 1. 模型可解释性和准确性的负相关关系
- 现状：  
随着模型复杂度增加、深度神经网络越深、模型预测能力和准确性高；但是于此同时可解释性、人类对机器的信任却逐步下降。

    |||||
    |---|---|---|---|
    | - |高|低|模型复杂度| 
    |低|？||
    |高||？|
    |可解释性|||

- 基于深度学习人工神经网络结构开发的AI工具，多数是以「**面向结果**」的方法训练的
- but，「可解释性」作为一种科学的框架，对于任何学科都不可或缺

### 2. 沙普利值x合作博弈，以及博弈交互理论x神经网络决策分析
省略

### 3. 人工智能在生物科学/医疗领域普及的时间预测
- 在基因编辑领域，已经大展拳脚这是毋庸置疑  
举例：DeepCRISPER模型，模型本身就是传统的Pre-training结合fine-tuning框架设计的，专门用于CHRISPER基因编辑技术领域的AI工具。  
不过生物基因表达本身就不算什么“白盒”，所以（个人思考）即便深度神经网络所提供的基因改造方案是“黑盒”决策的结果，那也没多少人敢站出来朝它丢石头。

- 在医学影像分析领域，疑似最可能有限被AI入侵的领域。  
因为基于CNN的图像处理和识别技术基本上已经很先进了。无论是Tesla的基于动态视觉识别的自动驾驶技术、Midjourney或Stable Diffusion的图像生成技术、还是遍布互联网的各种人脸识别技术，都已经足以证明图像输入分析和输出稳定性的AI技术已经足够强大且正在**商用**（本身也没什么成熟的「***行业监管***」）。  
只不过医疗不算是“商业机构”，所以推进的速度慢罢了。

### 4. 在金融/金融科技领域的应用预测
- 面向监管者的[`「规则」已经相对完善`](#13-监管的情况尤其是ai和fin-tech在我国) ，加上金融行业本身和资本密切挂钩，在响应新技术和新生产力上一定会优先行动。

- 公平和伦理问题：  
*举例，在保险金融市场，如果两个demographical info（性别、年龄、健康状况）几乎一样的人是否会被收取不同的**保险费（价格歧视发生）**？针对这种情况，美国保险法有规定“公司能够解释为什么拒保或者收取更高的保费”以确保客户对决策机制的知情权。*

### 5. 「数据的“血统”」，本书第7章的一个词
- 意义：特指被算法使用的信息来自哪里，用于向用户解释。
- 血统这个词的趣味：自带了以下属性
    - 人有选择“同意”个人数据被采集的自由，但实际上多数人是“被动同意”的
    - 数据来源可查 = 高贵；来源不可查 = 低贱
    - 存在有歧视性的敏感数据，以及此类数据的应用，e.g. 性别、学历、族裔、收入水平等信息在金融信贷领域的风险评估中，算法的“偏见”难以避免。

### 6. 消除AI模型中的“偏见”真的容易？
使用带有偏见或歧视性的数据 ⭢ 有偏见的AI；  
使用本来就有偏见的人类监管员 ⭢ 有偏见的AI；  
结论：偏见无法消除，甚至机器的偏见的消除可能比人类偏见消除更简单。

### 7. 目前流行的AI模型「评价」和「解释」工具推荐？
> 基于第9章推荐的「ExplainaBoard」工具的延伸和探索。

|工具 Util|Code|Key Feature|Note|Author|
|---|:---:|---|---|---|
|**ExplainaBoard** 本书译作“可解释排行榜”|*[`👉代码`](https://github.com/neulab/ExplainaBoard/blob/main/README.md)* |多维度分析、可交互（云端or本地均可）、在“评价”基础上增加“可解释性”的解读|热度：Github上仅300-stars（也许针对中文NLP模型）；更新：2年前|中国人研发Fu、Liu等人|
|**Captum**|*[`👉代码`](https://github.com/pytorch/captum/blob/master/README.md)*|用于PyTorch模型可解释性的库。目***前最贴合本书理论的实践工具***：因为它提供了一系列方法来将深度神经网络模型的输出归因于它们的输入特征，用于理解深度学习模型的决策过程。支持的技术包括集成梯度、梯度SHAP等。|热度：GitHub上4k-Stars；更新：2周前|Facebook团队开发|
|**ELI5**|*[`👉代码`](https://github.com/TeamHG-Memex/eli5?tab=readme-ov-file)*|一个Python库，帮助调试（debugging/inspecting）机器学习分类器并解释它们的预测。它支持多种传统机器学习框架和包：scikit-learn、Keras等。可视化输出预测和输入特征的重要关系|热度：GitHub上2.8k-stars；更新：5年前（传统）|某UK哥/UK团队吧（没查）|
|**AI Explainability 360（AIX360）**|*[`👉代码`](https://github.com/Trusted-AI/AIX360)*|一个工具包，提供了一整套算法。支持数据集和机器学习模型的解释性和可解释性（Interpretability and explainability）。它包含超过10个公平性指标和6种先进的解释性算法，可适用于多种数据类型和模型形态|热度：GitHub 1.6k-stars；更新：2个月前|IBM|
|**What-If Tool**|*[`👉代码`](https://github.com/pair-code/what-if-tool)*|交互式视觉界面，设计用来探索数据集和机器学习模型 expanding understanding of a black-box classification or regression ML model.|热度：GitHub 900-stars；更新：8个月前（发布于6年前）|Google团队|
|**Tf-explian**|*[`👉代码`](https://github.com/sicara/tf-explain)*|Interpretability Methods for tf.keras models with Tensorflow 2.x|热度：1k-stars；更新：2年前|Raphael Meudec（知名[`法国帅哥`](https://github.com/RaphaelMeudec)❓|
|**Lucid**|*[`👉代码`](https://github.com/tensorflow/lucid/blob/master/README.md)*|【已经关停了，可能是因为tensorflow1.0没人用了吧】A collection of infrastructure and tools for research in neural network interpretability.|热度：4.7k-stars；更新：2年前，2024年archived|tensorflow团队|
|**InterpretML**|*[`👉代码`](https://github.com/interpretml/interpret)*|Python开源库，Fit interpretable models|热度：6.2k；更新：8小时前|微软团队|
|**Alibi Explain**|*[`👉代码`](https://github.com/SeldonIO/alibi)*|Python开源库+1，Alibi is a Python library aimed at machine learning model inspection and interpretation. |热度：2.4k；更新：4个月前|UK的Seldon公司团队（ML研究商业机构）|
|**[`SHAP`](#19-shap-shapley-additive-explanations)**| [`SHAP-代码`](https://github.com/shap/shap)|尽管新的旧的工具千千万，但LIME和SHAP依然是模型解释领域的热门选择。它们泛用，因为可以解释任何ML（机器学习）模型，使其对人类可理解。|热度：22.4k-stars!!!；更新：8小时前（几乎实时）；流行max。|Scott M. Lundberg为首（曾任Microsoft研究员，现隶属GoogleDeepmind）|
|**[`LIME`](#18-limelocal-interpretable-model-agnostic-explanations)**|*[`LIME-代码`](https://github.com/marcotcr/lime)*|同上，Lime: Explaining the predictions of any machine learning classifier|热度：11.5k-stars!!; 更新：3年前|Marco Túlio Ribeiro为首，曾任Microsoft研究员，现隶属GoogleDeepmind，是SHAP代码发明人Scott的同校同专业学长👀|
|...|...|...|...|  

### 8. 基于深度神经网络的ML是「黑盒」，那「白盒」是？
基于规则、决策树、马尔可夫链模型、逻辑回归、等这些（CS50AI上都有教过）都是「白盒」。

### 9. AI系统的「解释」可以分为两大固定的方向
- 方向1: **内在（事前解释）**   
模型内在的解释、即把模型变成白盒。
- 方向2: **外在（事后解释）**  
黑盒模型之外，通过另一个“解释模型”来解释（e.g. 代理模型）
- *原因：  
即便把场景替换成人类和人类的决策，在心理学学术理解上，也有如下的说法：“人类有时候会通过谨慎、理性的推理做出决定，并且可以解释为什么做出某些决定；其他时候，则会先根据直觉做出决定，然后再为自己的决定找到解释，以支持或证明自己的决定。”。*

### 10. 已知「推荐系统」是一个黑盒模型（不可解释/不可信），用户是否会继续使用？
e.g.   
背景：某社媒平台的推荐算法，能够精准察觉并推荐令人沉迷的作品，但是这个推荐算法的可解释性很差。  
自媒体创作者角色：以逐利为导向的创作行为会难以“预测”，甚至由于推荐模型的不透明性，产生「推荐决策」是否公平、有歧视性、等质疑。  
普通浏览者角色：对「推荐内容」的选择性为零，无法通过某个交互界面管理和选择自己用于输入给推荐系统的数据。以信息获取为目的的浏览行为会因为对推荐机制的不信任而减少。


## 😵💡 基础行业知识整理（非AI）

### 1. 金融知识
#### 1.1 金融市场分类

| **按照交易标的物分类** | **按照发行和流通特性分类** | **按照交易是否有中介分类** | **按照交易方式分类** |
|:---|:---|:---|:---|
| 货币市场 | 初级市场 | 直接金融市场 | 现货市场 |
| 资本市场（股票、债券、基金等） | 次级市场 | 间接金融市场 | 期货市场 |
| 外汇市场 | 场外交易市场 |  | 期权市场 |
| 保险市场 |  |  |  |
| 黄金市场 |  |  |  |
| 其他投资品市场 |  |  |  |

#### 1.2 金融市场（在我国）的参与者有哪些？
|参与者|举例|
|---|---|
|监管机构和自律性组织|央行、银保监会、证监会……|
|银行|含政策性银行和普通商业银行|
|非银行金融机构|国有的保险公司、农信社、证券公司、证券交易中心、投资基金管理公司、财务公司……|
|外资/中外合资金融机构|外资/合资的银行、财务公司、保险公司|

#### 1.3 「监管」的情况（尤其是AI和Fin-tech在我国）
- 中国人民银行2019年，《金融科技fintech发展规划（2019-2021）》：“健全 **人工智能金融应用** 安全检测预警机制，研究制定 **人工智能金融应用** 监管规则，强化智能化金融工具安全认证，确保把 **人工智能金融应用** 规制在安全可控范围内”。
- 中国人民银行2021年，[`《人工智能算法金融应用 评价规范.pdf》`](http://www.hbbill.com/uploadFiles/-16/548/058/54/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E9%87%91%E8%9E%8D%E5%BA%94%E7%94%A8%E8%AF%84%E4%BB%B7%E8%A7%84%E8%8C%83%EF%BC%88JRT0221-2021%EF%BC%89.pdf)
- 大的监管规则有没有？
- 算法的评价及备案机制有没有？

#### 1.4 「量化策略」
- 1）一个完整的「**量化策略**」包含以下流程：

    |输入⭢|⭢策略处理逻辑⭢|⭢输出|
    |---|---|---|
    |Market Data、基本面数据、异常数据|选股、择时、仓位管理、止盈止损、等|推荐的交易、投资对象、其他行为|

- 2）与「**主动投资策略**」对立：  
主动选股 == 人性弱点（不严格或自私）、信息差、偏见；  
量化选滚 == 主动性、系统性、概率性 （**18世纪**以来就已经是热门的金融应用领域）。  

- 3）「**基于机器学习的量化投资模型**」和「传统量化投资模型」不同，后者省略，前者如下。

    |五大|环节|基于机器学习的量化投资模型|note|
    |---|---|---|---|
    |第一环节|特征工程 Feature Engineering|**挖掘额外信息和价值**|数据可用性评估、数据清洗、归一化、特征分析、降维升维等|
    |第二环节|搭建量化模型|以预测为目的，搭非线性+多因子预测模型，**消化大数据**|模型选择、过拟合、参数选择、等|
    |第三环节|优化算法|满足除了预测目的外的需求，**寻找最优解**，例如投资组合或特定约束条件下的投资|预期收益最大化、风险最小化、误差跟踪、交易成本最小化、交易量限制、个股权重设定等|
    |第四环节|回测调试|**指定时段进行模拟交易** ？为什么回测另量化人员头疼？样本分组不就好了？为什么要用“虚拟数据”来增加样本量。。。金融圈人材真的很爱“骗”🙄|年化收益率、夏普比率、最大回撤、信息比率、alpha、beta、等|
    |第五环节|交易执行|**实盘操作真实交易**|大单分拆、佣金费用、交易者预期/情绪管理、等|

- 4）量化投资 x AI 领域的几大**学派**（？也就是一种“投资风格”的百家争鸣现象吧
    - 学术量化派：AQR为代表。注重多因子策略做出多样化投资从而分散风险，有论文、有期刊、有专设图书馆等。
    - 因子挖掘派：WorldQuant为代表。注重数据源的挖掘，偏特征工程前期的部份。
    - 短线技术派：西蒙斯大奖章为代表。本书没讲。
    - Morgan Stanley下的MSCI公司有一套自己的“Barra风险因子”。本书没讲。
    - Microsoft团队开源了一项「AI量化回测框架 - Qlib」。本书没讲。

- 5）延伸阅读：关于***传统量化投资模型***  
    - 开端：「均值-方差模型」的提出开始的
    - 深化：「现代资产组合理论 Modern Portfolio Theory, MPT」
    - 1952年，以「均值-方差模型」为基础出版《资产选择：有效的多样化》Harry M. Markowit
    - 1959年，以「布朗运动」原理为基础提出《随机漫步理论》M.F.M Osborne
    - 1960-70年，「资本资产定价模型（CAPM）」
    - 1992年，用市场组合风险溢价、规模溢价、市净率溢价去研究股票收益率的「三因子模型」诞生，by Eugene F. Fama等，量化投资开始成熟问世。

#### 1.5 「自动化」x「金融」的经典案例故事：闪电崩盘
2010年5月6日、美国、美股，市值蒸发1万亿美元，道琼斯工业股均价在这次“闪电崩盘”中大跌1000点。原因是使用了自动执行算法，在短时间内大量抛售而引发。  
用途：说明技术在不成熟/不可控之前的“危害”。

#### 1.6 智能量化选股模型的「缺点」
- 如果历史数据（即训练数据）不俱参考价值，则预测结果效力减弱（e.g. 在“弱势有效市场下，基于历史数据信息的预测不可靠”
- 如果是算法是黑盒，模型有效性只能**后验**（术语叫做： Performance Attribution 业绩归因😆🙄），不能**先验**（即通过逻辑提前解释其可靠度）
- 收益来源难以具体准确计算，e.g. 二级市场上，这个投资组合具体赚的是什么钱？
- 模型是动态有效的吗。

#### 1.7 常见的两大类 Explainable AI 模型（此处带入金融场景）

||透明模型|Model Agnostic “[`代理模型`](#23-神经网络的表达结构)”|
|---|---|---|
||设计出可解释的模型|通过外部搭建可解释的代理模型、或事后解释|
||GAMxNN|[`LIME`](#18-limelocal-interpretable-model-agnostic-explanations)|
||GAM-INET|[`SHAP`](#19-shap-shapley-additive-explanations)|
||EBM|[`随机森林`](#110-随机森林算法)|
||NBDT||
||...|...|

#### 1.8 LIME（Local Interpretable Model-agnostic Explanations）
- 用途：LIME 的核心思想是对模型进行局部可解释（**局部可解释性**）。
- 原理：通过在模型做出预测的局部区域生成大量“扰动样本”数据，并观察模型的预测结果如何随输入的微小变化而变化，且拟合某个简单模型（如线性模型）来拟造和理解原模型的行为。
- 概括：照葫芦画瓢，但作为Model Agnostic算法，依然**可解释任意模型**
- 应用：
    - 金融信贷评估：在信贷评估模型中解释为何某客户的贷款申请被拒。
    - 欺诈检测：解释一个交易被标记为欺诈的具体原因。

#### 1.9 SHAP (SHapley Additive exPlanations)
就是基于沙普利值的一个**解释算法**，本书前章节有：[`SHAP算法`](#16-shap算法shapley-additive-explanations是什么)。此处概念略，但对比一下LIME和SHAP之区别😛：  
- 复杂度不同：  
SHAP是基于博弈论的“沙普利值”构建的算法，Lime则是一个简单的模仿算法。
- 解释范围不同：  
SHAP能做到全局可解释；LIME只做局部可解释。

#### 1.10 随机森林算法 Random Forest Algorithm
- 一句话：  
集成学习方法的一种，特别擅长于处理**分类**和**回归任务**。它由多棵决策树组成，通过集成多个决策树的预测结果来提高整体的预测精度和稳定性。  
- 用途：**分类**、**回归**、**拟合**。  
- 核心特点：  
    - 1. 集成学习（Ensemble Learning）  
    每棵树的预测结果被整合起来（例如，通过投票机制或平均），以得到更准确、更稳健的输出。  
    - 2. 决策树（Decision Trees）  
    随机森林的基础是**决策树**（一种简单但强大的学习方法）。决策树通过不断地将数据集分割成越来越小的子集来作决策，每个分割基于对特定特征的评估。
    - 3. 随机性（Randomness）  
    在构建每棵树时引入了随机性：增加模型的多样性，减少过拟合风险。
- 解释性（跟explainable AI 的联系）：  
尽管随机森林是一个复合模型，但它的解释性相对较好，原因包括：
    - 1. 特征重要性（Feature Importance）  
    随机森林能够提供关于哪些特征对预测结果影响最大的量化度量。特征重要性通常是通过观察当某个特征在决策树中被用作分割点时，它在减少模型误差方面的效果来评估的。
    - 2. 模型透明度  
    每棵单独的树都是完全可解释的。
- 应用场景：  
随机森林在**金融领域**的应用非常广泛：
    - 信用评分：评估借款人的信用风险。
    - 欺诈检测：识别和预防金融欺诈行为。
    - 投资策略：模型市场行为和制定交易策略。

#### 1.11 
1. 资源配置：金融市场主要功能之一
2. 利率市场化：信用好的人利率低、信用差的客户利率高
3. 客户违约风险预测系统：贷款机构数字化转型101
4. 资管产品的多层嵌套：资管新规发布其禁令————监管层不希望看见过复杂过难以把控的设计（同理，黑盒AI也是），不然就是美国次贷危机again
5. 金融人工智能的落地场景：智能风控、智能投研、智能营销、智能客服、智能保险、智能监管（合规模型、审计模型、反洗钱模型、反欺诈模型）……
6. 监管方向（之一）：是对「可解释性」的监管

#### 1.12 洗钱（Money Laundry）和反洗钱
1. 洗钱三阶段

|Layering 处置/分账|Placement 培植|Integration 融合|
|---|---|---|
|多层次复杂的转账交易，使非法所得钱财脱离来源|把非法所得花式存入金融机构|以合法的转账交易为掩护，隐瞒脏钱|

2. 反洗钱系统/模型
本书没介绍……😓

### 2. 计算机视觉 x AI
这一章写的很差，略。

### 3. 自然语言处理（NLP）x AI
#### 3.1 两种机器学习模型
- 基于「特征工程」的传统机器学习模型，如SVM。  
可解释性-强；功能性-弱
- 基于神经网络的深度学习模型，如CNN、RNN、LSTM。  
可解释性-弱；功能性-强

#### 3.2 NLP的可解释性研究，目前两大方向
- 结构分析  
- 行文分析  

## 结尾
应用的部份有些写得好有些写得（不能称作“不好”）落后于时代。2～3年的时间对于AI领域来说很多实践与应用都变化得太快了。  
至少在现实生活中，各个公司（Open AI、Cluade、Sona、谷歌、甚至马斯克得科技公司xAI……）都在卷谁家模型最大、谁家参数最多、谁的芯片算力更强。似乎没什么声量在“关注AI安全和可解释性”上面。  
也许时间还没到：安全问题应该在技术革新的周期当中处在一个“跟随着生产力提升之后”的阶段才会自然涌现（说人话就是出事之后才会有人重视、才会有市场的“需求”、和资本的“可收益性”）。
